# llama-stack-mcp

## Running the Server

To run the llama stack server, use:

```sh
llama stack run run.yml
```

## Interacting with the Server

Once the server is running, you can interact with it using HTTP requests (e.g., with `curl` or Postman) to the server's API endpoints. Refer to the API documentation or the `run.yml` configuration for available endpoints and usage examples.

## Running a Python File

To run the Python file, use:

```sh
python prompt.py
```

This should produce the following output:

```
Hello! How can I assist you today?
```

Replace `prompt.py` with the name of your Python script if different.
