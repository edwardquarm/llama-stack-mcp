# llama-stack-mcp

## Running the Server

To run the llama stack server, use:

```sh
llama stack run run.yml
```

## Interacting with the Server

Once the server is running, you can interact with it using HTTP requests (e.g., with `curl` or Postman) to the server's API endpoints. Refer to the API documentation or the `run.yml` configuration for available endpoints and usage examples.

## Running a Python File

To run a Python file, use:

```sh
python prompt.py
```

Replace `prompt.py` with the name of your Python script if different.
